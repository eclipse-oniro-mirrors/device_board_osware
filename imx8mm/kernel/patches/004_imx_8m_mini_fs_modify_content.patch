commit 6aedc2dfa2b36653c59a6e651d0e72b7dae4796d
Author: zhaoxc0502 <zhaoxc0502@thundersoft.com>
Date:   Wed Jun 1 11:58:01 2022 +0800

    modify kernel fs for imx8m mini
    
    Change-Id: I5ea5ad3235ed61d6312d32f5671dc3a362b70233

diff --git a/fs/aio.c b/fs/aio.c
index 5e5333d72..bd182bcca 100644
--- a/fs/aio.c
+++ b/fs/aio.c
@@ -182,9 +182,8 @@ struct poll_iocb {
 	struct file		*file;
 	struct wait_queue_head	*head;
 	__poll_t		events;
+	bool			done;
 	bool			cancelled;
-	bool			work_scheduled;
-	bool			work_need_resched;
 	struct wait_queue_entry	wait;
 	struct work_struct	work;
 };
@@ -1622,51 +1621,6 @@ static void aio_poll_put_work(struct work_struct *work)
 	iocb_put(iocb);
 }
 
-/*
- * Safely lock the waitqueue which the request is on, synchronizing with the
- * case where the ->poll() provider decides to free its waitqueue early.
- *
- * Returns true on success, meaning that req->head->lock was locked, req->wait
- * is on req->head, and an RCU read lock was taken.  Returns false if the
- * request was already removed from its waitqueue (which might no longer exist).
- */
-static bool poll_iocb_lock_wq(struct poll_iocb *req)
-{
-	wait_queue_head_t *head;
-
-	/*
-	 * While we hold the waitqueue lock and the waitqueue is nonempty,
-	 * wake_up_pollfree() will wait for us.  However, taking the waitqueue
-	 * lock in the first place can race with the waitqueue being freed.
-	 *
-	 * We solve this as eventpoll does: by taking advantage of the fact that
-	 * all users of wake_up_pollfree() will RCU-delay the actual free.  If
-	 * we enter rcu_read_lock() and see that the pointer to the queue is
-	 * non-NULL, we can then lock it without the memory being freed out from
-	 * under us, then check whether the request is still on the queue.
-	 *
-	 * Keep holding rcu_read_lock() as long as we hold the queue lock, in
-	 * case the caller deletes the entry from the queue, leaving it empty.
-	 * In that case, only RCU prevents the queue memory from being freed.
-	 */
-	rcu_read_lock();
-	head = smp_load_acquire(&req->head);
-	if (head) {
-		spin_lock(&head->lock);
-		if (!list_empty(&req->wait.entry))
-			return true;
-		spin_unlock(&head->lock);
-	}
-	rcu_read_unlock();
-	return false;
-}
-
-static void poll_iocb_unlock_wq(struct poll_iocb *req)
-{
-	spin_unlock(&req->head->lock);
-	rcu_read_unlock();
-}
-
 static void aio_poll_complete_work(struct work_struct *work)
 {
 	struct poll_iocb *req = container_of(work, struct poll_iocb, work);
@@ -1686,27 +1640,14 @@ static void aio_poll_complete_work(struct work_struct *work)
 	 * avoid further branches in the fast path.
 	 */
 	spin_lock_irq(&ctx->ctx_lock);
-	if (poll_iocb_lock_wq(req)) {
-		if (!mask && !READ_ONCE(req->cancelled)) {
-			/*
-			 * The request isn't actually ready to be completed yet.
-			 * Reschedule completion if another wakeup came in.
-			 */
-			if (req->work_need_resched) {
-				schedule_work(&req->work);
-				req->work_need_resched = false;
-			} else {
-				req->work_scheduled = false;
-			}
-			poll_iocb_unlock_wq(req);
-			spin_unlock_irq(&ctx->ctx_lock);
-			return;
-		}
-		list_del_init(&req->wait.entry);
-		poll_iocb_unlock_wq(req);
-	} /* else, POLLFREE has freed the waitqueue, so we must complete */
+	if (!mask && !READ_ONCE(req->cancelled)) {
+		add_wait_queue(req->head, &req->wait);
+		spin_unlock_irq(&ctx->ctx_lock);
+		return;
+	}
 	list_del_init(&iocb->ki_list);
 	iocb->ki_res.res = mangle_poll(mask);
+	req->done = true;
 	spin_unlock_irq(&ctx->ctx_lock);
 
 	iocb_put(iocb);
@@ -1718,14 +1659,13 @@ static int aio_poll_cancel(struct kiocb *iocb)
 	struct aio_kiocb *aiocb = container_of(iocb, struct aio_kiocb, rw);
 	struct poll_iocb *req = &aiocb->poll;
 
-	if (poll_iocb_lock_wq(req)) {
-		WRITE_ONCE(req->cancelled, true);
-		if (!req->work_scheduled) {
-			schedule_work(&aiocb->poll.work);
-			req->work_scheduled = true;
-		}
-		poll_iocb_unlock_wq(req);
-	} /* else, the request was force-cancelled by POLLFREE already */
+	spin_lock(&req->head->lock);
+	WRITE_ONCE(req->cancelled, true);
+	if (!list_empty(&req->wait.entry)) {
+		list_del_init(&req->wait.entry);
+		schedule_work(&aiocb->poll.work);
+	}
+	spin_unlock(&req->head->lock);
 
 	return 0;
 }
@@ -1742,26 +1682,20 @@ static int aio_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,
 	if (mask && !(mask & req->events))
 		return 0;
 
-	/*
-	 * Complete the request inline if possible.  This requires that three
-	 * conditions be met:
-	 *   1. An event mask must have been passed.  If a plain wakeup was done
-	 *	instead, then mask == 0 and we have to call vfs_poll() to get
-	 *	the events, so inline completion isn't possible.
-	 *   2. The completion work must not have already been scheduled.
-	 *   3. ctx_lock must not be busy.  We have to use trylock because we
-	 *	already hold the waitqueue lock, so this inverts the normal
-	 *	locking order.  Use irqsave/irqrestore because not all
-	 *	filesystems (e.g. fuse) call this function with IRQs disabled,
-	 *	yet IRQs have to be disabled before ctx_lock is obtained.
-	 */
-	if (mask && !req->work_scheduled &&
-	    spin_trylock_irqsave(&iocb->ki_ctx->ctx_lock, flags)) {
+	list_del_init(&req->wait.entry);
+
+	if (mask && spin_trylock_irqsave(&iocb->ki_ctx->ctx_lock, flags)) {
 		struct kioctx *ctx = iocb->ki_ctx;
 
-		list_del_init(&req->wait.entry);
+		/*
+		 * Try to complete the iocb inline if we can. Use
+		 * irqsave/irqrestore because not all filesystems (e.g. fuse)
+		 * call this function with IRQs disabled and because IRQs
+		 * have to be disabled before ctx_lock is obtained.
+		 */
 		list_del(&iocb->ki_list);
 		iocb->ki_res.res = mangle_poll(mask);
+		req->done = true;
 		if (iocb->ki_eventfd && eventfd_signal_count()) {
 			iocb = NULL;
 			INIT_WORK(&req->work, aio_poll_put_work);
@@ -1771,43 +1705,7 @@ static int aio_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,
 		if (iocb)
 			iocb_put(iocb);
 	} else {
-		/*
-		 * Schedule the completion work if needed.  If it was already
-		 * scheduled, record that another wakeup came in.
-		 *
-		 * Don't remove the request from the waitqueue here, as it might
-		 * not actually be complete yet (we won't know until vfs_poll()
-		 * is called), and we must not miss any wakeups.  POLLFREE is an
-		 * exception to this; see below.
-		 */
-		if (req->work_scheduled) {
-			req->work_need_resched = true;
-		} else {
-			schedule_work(&req->work);
-			req->work_scheduled = true;
-		}
-
-		/*
-		 * If the waitqueue is being freed early but we can't complete
-		 * the request inline, we have to tear down the request as best
-		 * we can.  That means immediately removing the request from its
-		 * waitqueue and preventing all further accesses to the
-		 * waitqueue via the request.  We also need to schedule the
-		 * completion work (done above).  Also mark the request as
-		 * cancelled, to potentially skip an unneeded call to ->poll().
-		 */
-		if (mask & POLLFREE) {
-			WRITE_ONCE(req->cancelled, true);
-			list_del_init(&req->wait.entry);
-
-			/*
-			 * Careful: this *must* be the last step, since as soon
-			 * as req->head is NULL'ed out, the request can be
-			 * completed and freed, since aio_poll_complete_work()
-			 * will no longer need to take the waitqueue lock.
-			 */
-			smp_store_release(&req->head, NULL);
-		}
+		schedule_work(&req->work);
 	}
 	return 1;
 }
@@ -1815,7 +1713,6 @@ static int aio_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,
 struct aio_poll_table {
 	struct poll_table_struct	pt;
 	struct aio_kiocb		*iocb;
-	bool				queued;
 	int				error;
 };
 
@@ -1826,12 +1723,11 @@ aio_poll_queue_proc(struct file *file, struct wait_queue_head *head,
 	struct aio_poll_table *pt = container_of(p, struct aio_poll_table, pt);
 
 	/* multiple wait queues per file are not supported */
-	if (unlikely(pt->queued)) {
+	if (unlikely(pt->iocb->poll.head)) {
 		pt->error = -EINVAL;
 		return;
 	}
 
-	pt->queued = true;
 	pt->error = 0;
 	pt->iocb->poll.head = head;
 	add_wait_queue(head, &pt->iocb->poll.wait);
@@ -1856,14 +1752,12 @@ static int aio_poll(struct aio_kiocb *aiocb, const struct iocb *iocb)
 	req->events = demangle_poll(iocb->aio_buf) | EPOLLERR | EPOLLHUP;
 
 	req->head = NULL;
+	req->done = false;
 	req->cancelled = false;
-	req->work_scheduled = false;
-	req->work_need_resched = false;
 
 	apt.pt._qproc = aio_poll_queue_proc;
 	apt.pt._key = req->events;
 	apt.iocb = aiocb;
-	apt.queued = false;
 	apt.error = -EINVAL; /* same as no support for IOCB_CMD_POLL */
 
 	/* initialized the list so that we can do list_empty checks */
@@ -1872,35 +1766,23 @@ static int aio_poll(struct aio_kiocb *aiocb, const struct iocb *iocb)
 
 	mask = vfs_poll(req->file, &apt.pt) & req->events;
 	spin_lock_irq(&ctx->ctx_lock);
-	if (likely(apt.queued)) {
-		bool on_queue = poll_iocb_lock_wq(req);
-
-		if (!on_queue || req->work_scheduled) {
-			/*
-			 * aio_poll_wake() already either scheduled the async
-			 * completion work, or completed the request inline.
-			 */
-			if (apt.error) /* unsupported case: multiple queues */
+	if (likely(req->head)) {
+		spin_lock(&req->head->lock);
+		if (unlikely(list_empty(&req->wait.entry))) {
+			if (apt.error)
 				cancel = true;
 			apt.error = 0;
 			mask = 0;
 		}
 		if (mask || apt.error) {
-			/* Steal to complete synchronously. */
 			list_del_init(&req->wait.entry);
 		} else if (cancel) {
-			/* Cancel if possible (may be too late though). */
 			WRITE_ONCE(req->cancelled, true);
-		} else if (on_queue) {
-			/*
-			 * Actually waiting for an event, so add the request to
-			 * active_reqs so that it can be cancelled if needed.
-			 */
+		} else if (!req->done) { /* actually waiting for an event */
 			list_add_tail(&aiocb->ki_list, &ctx->active_reqs);
 			aiocb->ki_cancel = aio_poll_cancel;
 		}
-		if (on_queue)
-			poll_iocb_unlock_wq(req);
+		spin_unlock(&req->head->lock);
 	}
 	if (mask) { /* no async, we'd stolen it */
 		aiocb->ki_res.res = mangle_poll(mask);
diff --git a/fs/cifs/smb2ops.c b/fs/cifs/smb2ops.c
index 0e8f48403..fdb1d660b 100644
--- a/fs/cifs/smb2ops.c
+++ b/fs/cifs/smb2ops.c
@@ -1526,7 +1526,6 @@ smb2_ioctl_query_info(const unsigned int xid,
 	unsigned int size[2];
 	void *data[2];
 	int create_options = is_dir ? CREATE_NOT_FILE : CREATE_NOT_DIR;
-	void (*free_req1_func)(struct smb_rqst *r);
 
 	vars = kzalloc(sizeof(*vars), GFP_ATOMIC);
 	if (vars == NULL)
@@ -1536,29 +1535,27 @@ smb2_ioctl_query_info(const unsigned int xid,
 
 	resp_buftype[0] = resp_buftype[1] = resp_buftype[2] = CIFS_NO_BUFFER;
 
-	if (copy_from_user(&qi, arg, sizeof(struct smb_query_info))) {
-		rc = -EFAULT;
-		goto free_vars;
-	}
+	if (copy_from_user(&qi, arg, sizeof(struct smb_query_info)))
+		goto e_fault;
+
 	if (qi.output_buffer_length > 1024) {
-		rc = -EINVAL;
-		goto free_vars;
+		kfree(vars);
+		return -EINVAL;
 	}
 
 	if (!ses || !server) {
-		rc = -EIO;
-		goto free_vars;
+		kfree(vars);
+		return -EIO;
 	}
 
 	if (smb3_encryption_required(tcon))
 		flags |= CIFS_TRANSFORM_REQ;
 
-	if (qi.output_buffer_length) {
-		buffer = memdup_user(arg + sizeof(struct smb_query_info), qi.output_buffer_length);
-		if (IS_ERR(buffer)) {
-			rc = PTR_ERR(buffer);
-			goto free_vars;
-		}
+	buffer = memdup_user(arg + sizeof(struct smb_query_info),
+			     qi.output_buffer_length);
+	if (IS_ERR(buffer)) {
+		kfree(vars);
+		return PTR_ERR(buffer);
 	}
 
 	/* Open */
@@ -1596,45 +1593,45 @@ smb2_ioctl_query_info(const unsigned int xid,
 	rc = SMB2_open_init(tcon, server,
 			    &rqst[0], &oplock, &oparms, path);
 	if (rc)
-		goto free_output_buffer;
+		goto iqinf_exit;
 	smb2_set_next_command(tcon, &rqst[0]);
 
 	/* Query */
 	if (qi.flags & PASSTHRU_FSCTL) {
 		/* Can eventually relax perm check since server enforces too */
-		if (!capable(CAP_SYS_ADMIN)) {
+		if (!capable(CAP_SYS_ADMIN))
 			rc = -EPERM;
-			goto free_open_req;
+		else  {
+			rqst[1].rq_iov = &vars->io_iov[0];
+			rqst[1].rq_nvec = SMB2_IOCTL_IOV_SIZE;
+
+			rc = SMB2_ioctl_init(tcon, server,
+					     &rqst[1],
+					     COMPOUND_FID, COMPOUND_FID,
+					     qi.info_type, true, buffer,
+					     qi.output_buffer_length,
+					     CIFSMaxBufSize -
+					     MAX_SMB2_CREATE_RESPONSE_SIZE -
+					     MAX_SMB2_CLOSE_RESPONSE_SIZE);
 		}
-		rqst[1].rq_iov = &vars->io_iov[0];
-		rqst[1].rq_nvec = SMB2_IOCTL_IOV_SIZE;
-
-		rc = SMB2_ioctl_init(tcon, server, &rqst[1], COMPOUND_FID, COMPOUND_FID,
-				     qi.info_type, true, buffer, qi.output_buffer_length,
-				     CIFSMaxBufSize - MAX_SMB2_CREATE_RESPONSE_SIZE -
-				     MAX_SMB2_CLOSE_RESPONSE_SIZE);
-		free_req1_func = SMB2_ioctl_free;
 	} else if (qi.flags == PASSTHRU_SET_INFO) {
 		/* Can eventually relax perm check since server enforces too */
-		if (!capable(CAP_SYS_ADMIN)) {
+		if (!capable(CAP_SYS_ADMIN))
 			rc = -EPERM;
-			goto free_open_req;
-		}
-		if (qi.output_buffer_length < 8) {
-			rc = -EINVAL;
-			goto free_open_req;
-		}
-		rqst[1].rq_iov = &vars->si_iov[0];
-		rqst[1].rq_nvec = 1;
-
-		/* MS-FSCC 2.4.13 FileEndOfFileInformation */
-		size[0] = 8;
-		data[0] = buffer;
-
-		rc = SMB2_set_info_init(tcon, server, &rqst[1], COMPOUND_FID, COMPOUND_FID,
-					current->tgid, FILE_END_OF_FILE_INFORMATION,
+		else  {
+			rqst[1].rq_iov = &vars->si_iov[0];
+			rqst[1].rq_nvec = 1;
+
+			size[0] = 8;
+			data[0] = buffer;
+
+			rc = SMB2_set_info_init(tcon, server,
+					&rqst[1],
+					COMPOUND_FID, COMPOUND_FID,
+					current->tgid,
+					FILE_END_OF_FILE_INFORMATION,
 					SMB2_O_INFO_FILE, 0, data, size);
-		free_req1_func = SMB2_set_info_free;
+		}
 	} else if (qi.flags == PASSTHRU_QUERY_INFO) {
 		rqst[1].rq_iov = &vars->qi_iov[0];
 		rqst[1].rq_nvec = 1;
@@ -1645,7 +1642,6 @@ smb2_ioctl_query_info(const unsigned int xid,
 				  qi.info_type, qi.additional_information,
 				  qi.input_buffer_length,
 				  qi.output_buffer_length, buffer);
-		free_req1_func = SMB2_query_info_free;
 	} else { /* unknown flags */
 		cifs_tcon_dbg(VFS, "Invalid passthru query flags: 0x%x\n",
 			      qi.flags);
@@ -1653,7 +1649,7 @@ smb2_ioctl_query_info(const unsigned int xid,
 	}
 
 	if (rc)
-		goto free_open_req;
+		goto iqinf_exit;
 	smb2_set_next_command(tcon, &rqst[1]);
 	smb2_set_related(&rqst[1]);
 
@@ -1664,14 +1660,14 @@ smb2_ioctl_query_info(const unsigned int xid,
 	rc = SMB2_close_init(tcon, server,
 			     &rqst[2], COMPOUND_FID, COMPOUND_FID, false);
 	if (rc)
-		goto free_req_1;
+		goto iqinf_exit;
 	smb2_set_related(&rqst[2]);
 
 	rc = compound_send_recv(xid, ses, server,
 				flags, 3, rqst,
 				resp_buftype, rsp_iov);
 	if (rc)
-		goto out;
+		goto iqinf_exit;
 
 	/* No need to bump num_remote_opens since handle immediately closed */
 	if (qi.flags & PASSTHRU_FSCTL) {
@@ -1681,22 +1677,18 @@ smb2_ioctl_query_info(const unsigned int xid,
 			qi.input_buffer_length = le32_to_cpu(io_rsp->OutputCount);
 		if (qi.input_buffer_length > 0 &&
 		    le32_to_cpu(io_rsp->OutputOffset) + qi.input_buffer_length
-		    > rsp_iov[1].iov_len) {
-			rc = -EFAULT;
-			goto out;
-		}
+		    > rsp_iov[1].iov_len)
+			goto e_fault;
 
 		if (copy_to_user(&pqi->input_buffer_length,
 				 &qi.input_buffer_length,
-				 sizeof(qi.input_buffer_length))) {
-			rc = -EFAULT;
-			goto out;
-		}
+				 sizeof(qi.input_buffer_length)))
+			goto e_fault;
 
 		if (copy_to_user((void __user *)pqi + sizeof(struct smb_query_info),
 				 (const void *)io_rsp + le32_to_cpu(io_rsp->OutputOffset),
 				 qi.input_buffer_length))
-			rc = -EFAULT;
+			goto e_fault;
 	} else {
 		pqi = (struct smb_query_info __user *)arg;
 		qi_rsp = (struct smb2_query_info_rsp *)rsp_iov[1].iov_base;
@@ -1704,30 +1696,28 @@ smb2_ioctl_query_info(const unsigned int xid,
 			qi.input_buffer_length = le32_to_cpu(qi_rsp->OutputBufferLength);
 		if (copy_to_user(&pqi->input_buffer_length,
 				 &qi.input_buffer_length,
-				 sizeof(qi.input_buffer_length))) {
-			rc = -EFAULT;
-			goto out;
-		}
+				 sizeof(qi.input_buffer_length)))
+			goto e_fault;
 
 		if (copy_to_user(pqi + 1, qi_rsp->Buffer,
 				 qi.input_buffer_length))
-			rc = -EFAULT;
+			goto e_fault;
 	}
 
-out:
+ iqinf_exit:
+	cifs_small_buf_release(rqst[0].rq_iov[0].iov_base);
+	cifs_small_buf_release(rqst[1].rq_iov[0].iov_base);
+	cifs_small_buf_release(rqst[2].rq_iov[0].iov_base);
 	free_rsp_buf(resp_buftype[0], rsp_iov[0].iov_base);
 	free_rsp_buf(resp_buftype[1], rsp_iov[1].iov_base);
 	free_rsp_buf(resp_buftype[2], rsp_iov[2].iov_base);
-	SMB2_close_free(&rqst[2]);
-free_req_1:
-	free_req1_func(&rqst[1]);
-free_open_req:
-	SMB2_open_free(&rqst[0]);
-free_output_buffer:
-	kfree(buffer);
-free_vars:
 	kfree(vars);
+	kfree(buffer);
 	return rc;
+
+e_fault:
+	rc = -EFAULT;
+	goto iqinf_exit;
 }
 
 static ssize_t
diff --git a/fs/file_table.c b/fs/file_table.c
index 7a3b4a7f6..709ada315 100644
--- a/fs/file_table.c
+++ b/fs/file_table.c
@@ -376,7 +376,6 @@ void __fput_sync(struct file *file)
 }
 
 EXPORT_SYMBOL(fput);
-EXPORT_SYMBOL(__fput_sync);
 
 void __init files_init(void)
 {
diff --git a/fs/hmdfs/Makefile b/fs/hmdfs/Makefile
index 20896e716..48a64acc8 100644
--- a/fs/hmdfs/Makefile
+++ b/fs/hmdfs/Makefile
@@ -5,7 +5,6 @@ hmdfs-y := main.o super.o inode.o dentry.o inode_root.o file_merge.o
 hmdfs-y += hmdfs_client.o hmdfs_server.o inode_local.o inode_remote.o
 hmdfs-y += inode_merge.o hmdfs_dentryfile.o file_root.o file_remote.o
 hmdfs-y += file_local.o client_writeback.o server_writeback.o stash.o
-hmdfs-y += hmdfs_share.o
 
 hmdfs-y += comm/device_node.o comm/message_verify.o comm/node_cb.o
 hmdfs-y += comm/connection.o comm/socket_adapter.o comm/transport.o
diff --git a/fs/hmdfs/file_local.c b/fs/hmdfs/file_local.c
index 47f39e392..2b9c64a82 100644
--- a/fs/hmdfs/file_local.c
+++ b/fs/hmdfs/file_local.c
@@ -17,7 +17,6 @@
 #include "hmdfs_dentryfile.h"
 #include "hmdfs_device_view.h"
 #include "hmdfs_merge_view.h"
-#include "hmdfs_share.h"
 #include "hmdfs_trace.h"
 
 int hmdfs_file_open_local(struct inode *inode, struct file *file)
@@ -275,80 +274,185 @@ static int hmdfs_dir_release_local(struct inode *inode, struct file *file)
 	return 0;
 }
 
-const struct file_operations hmdfs_dir_ops_local = {
-	.owner = THIS_MODULE,
-	.iterate = hmdfs_iterate_local,
-	.open = hmdfs_dir_open_local,
-	.release = hmdfs_dir_release_local,
-	.fsync = hmdfs_fsync_local,
-};
+static inline bool hmdfs_is_dst_path(struct path *src, struct path *dst)
+{
+	return (src->dentry == dst->dentry) && (src->mnt == dst->mnt);
+}
 
-static int __hmdfs_ioc_set_share_path(struct file *file,
-					struct hmdfs_share_control *sc)
+bool hmdfs_is_share_file(struct file *file)
 {
-	struct super_block *sb = file->f_inode->i_sb;
-	struct hmdfs_sb_info *sbi = hmdfs_sb(sb);
+	struct file *cur_file = file;
+	struct hmdfs_dentry_info *gdi;
+	struct hmdfs_file_info *gfi;
+
+	while (cur_file->f_inode->i_sb->s_magic == HMDFS_SUPER_MAGIC) {
+		gdi = hmdfs_d(cur_file->f_path.dentry);
+		gfi = hmdfs_f(cur_file);
+		if (hm_isshare(gdi->file_type))
+			return true;
+		if (gfi->lower_file)
+			cur_file = gfi->lower_file;
+		else
+			break;
+	}
+
+	return false;
+}
+
+bool hmdfs_is_share_item_still_valid(struct hmdfs_share_item *item)
+{
+	if (kref_read(&item->ref) == 1 && time_after(jiffies, item->timeout))
+		return false;
+
+	return true;
+}
+
+inline void release_share_item(struct hmdfs_share_item *item)
+{
+	kfree(item->relative_path.name);
+	fput(item->file);
+	kfree(item);
+}
+
+void hmdfs_remove_share_item(struct kref *ref)
+{
+	struct hmdfs_share_item *item =
+			container_of(ref, struct hmdfs_share_item, ref);
+
+	list_del(&item->list);
+	release_share_item(item);
+}
+
+struct hmdfs_share_item *hmdfs_lookup_share_item(struct hmdfs_share_table *st,
+						struct qstr *cur_relative_path)
+{
+	struct hmdfs_share_item *item, *tmp;
+
+	list_for_each_entry_safe(item, tmp, &st->item_list_head, list) {
+		if (hmdfs_is_share_item_still_valid(item)) {
+			if (qstr_eq(&item->relative_path, cur_relative_path))
+				return item;
+		} else {
+			kref_put(&item->ref, hmdfs_remove_share_item);
+			st->item_cnt--;
+		}
+	}
+
+	return NULL;
+}
+
+inline void set_item_timeout(struct hmdfs_share_item *item)
+{
+	item->timeout = jiffies + HZ * HMDFS_SHARE_ITEM_TIMEOUT_S;
+}
+
+static int hmdfs_insert_share_item(struct hmdfs_share_table *st,
+		struct qstr *relative_path, struct file *file, char *cid)
+{
+	struct hmdfs_share_item *new_item = NULL;
+	int ret = 0;
+
+	if (st->item_cnt >= st->max_cnt) {
+		ret = -EMFILE;
+		goto out;
+	}
+
+	new_item = kmalloc(sizeof(*new_item), GFP_KERNEL);
+	if (new_item) {
+		new_item->file = file;
+		get_file(file);
+		new_item->relative_path = *relative_path;
+		memcpy(new_item->cid, cid, HMDFS_CID_SIZE);
+		kref_init(&new_item->ref);
+		list_add_tail(&new_item->list, &st->item_list_head);
+		set_item_timeout(new_item);
+		st->item_cnt++;
+	} else {
+		ret = -ENOMEM;
+	}
+
+out:
+	return ret;
+}
+
+static int hmdfs_update_share_item(struct hmdfs_share_item *item,
+					struct file *file, char *cid)
+{
+	/* if not the same file, we need to update struct file */
+	if (!hmdfs_is_dst_path(&file->f_path, &item->file->f_path)) {
+		fput(item->file);
+		item->file = file;
+		get_file(file);
+	}
+	memcpy(item->cid, cid, HMDFS_CID_SIZE);
+	set_item_timeout(item);
+
+	return 0;
+}
+
+static int hmdfs_add_to_share_table(struct file *file,
+		struct hmdfs_sb_info *sbi, struct hmdfs_share_control *sc)
+{
+	struct fd src = fdget(sc->src_fd);
 	struct hmdfs_share_table *st = &sbi->share_table;
 	struct hmdfs_share_item *item;
 	struct dentry *dentry;
-	const char *dir_path, *full_path;
+	const char *dir_path, *cur_path;
 	struct qstr relative_path;
-	struct fd src;
 	int err = 0;
 
-	src = fdget(sc->src_fd);
 	if (!src.file)
 		return -EBADF;
 
-	/* only reg file can be shared */
 	if (!S_ISREG(src.file->f_inode->i_mode)) {
 		err = -EPERM;
 		goto err_out;
 	}
 
-	/* share file is not allowed to be shared */
 	if (hmdfs_is_share_file(src.file)) {
 		err = -EPERM;
 		goto err_out;
 	}
 
+	dir_path = hmdfs_get_dentry_relative_path(file->f_path.dentry);
+	if (unlikely(!dir_path)) {
+		err = -ENOMEM;
+		goto err_out;
+	}
+
 	dentry = src.file->f_path.dentry;
 	if (dentry->d_name.len > NAME_MAX) {
+		kfree(dir_path);
 		err = -ENAMETOOLONG;
 		goto err_out;
 	}
 
-	dir_path = hmdfs_get_dentry_relative_path(file->f_path.dentry);
-	if (unlikely(!dir_path)) {
+	cur_path = hmdfs_connect_path(dir_path, dentry->d_name.name);
+	if (unlikely(!cur_path)) {
+		kfree(dir_path);
 		err = -ENOMEM;
 		goto err_out;
 	}
-
-	full_path = hmdfs_connect_path(dir_path, dentry->d_name.name);
-	if (unlikely(!full_path)) {
-		err = -ENOMEM;
-		goto free_dir;
-	}
-	relative_path.name = full_path;
-	relative_path.len = strlen(full_path);
+	relative_path.name = cur_path;
+	relative_path.len = strlen(cur_path);
 
 	spin_lock(&sbi->share_table.item_list_lock);
 	item = hmdfs_lookup_share_item(st, &relative_path);
-	if (!item) {
-		err = insert_share_item(st, &relative_path, src.file, sc->cid);
-		goto unlock;
+	if (!item)
+		err = hmdfs_insert_share_item(st, &relative_path,
+							src.file, sc->cid);
+	else {
+		if (kref_read(&item->ref) != 1)
+			err = -EEXIST;
+		else
+			hmdfs_update_share_item(item, src.file, sc->cid);
 	}
-
-	if (item->opened)
-		err = -EEXIST;
-	else
-		update_share_item(item, src.file, sc->cid);
-
-unlock:
 	spin_unlock(&sbi->share_table.item_list_lock);
-	kfree(full_path);
-free_dir:
+
+	if (err < 0)
+		kfree(cur_path);
 	kfree(dir_path);
+
 err_out:
 	fdput(src);
 	return err;
@@ -357,16 +461,21 @@ static int __hmdfs_ioc_set_share_path(struct file *file,
 static int hmdfs_ioc_set_share_path(struct file *file, unsigned long arg)
 {
 	struct hmdfs_share_control sc;
+	struct super_block *sb = file->f_inode->i_sb;
+	struct hmdfs_sb_info *sbi = hmdfs_sb(sb);
+	int error;
 
 	if (copy_from_user(&sc, (struct hmdfs_share_control __user *)arg,
-			sizeof(sc)))
+							sizeof(sc)))
 		return -EFAULT;
 
-	return __hmdfs_ioc_set_share_path(file, &sc);
+	error = hmdfs_add_to_share_table(file, sbi, &sc);
+
+	return error;
 }
 
 static long hmdfs_dir_ioctl_local(struct file *file, unsigned int cmd,
-				unsigned long arg)
+						unsigned long arg)
 {
 	switch (cmd) {
 	case HMDFS_IOC_SET_SHARE_PATH:
@@ -376,6 +485,14 @@ static long hmdfs_dir_ioctl_local(struct file *file, unsigned int cmd,
 	}
 }
 
+const struct file_operations hmdfs_dir_ops_local = {
+	.owner = THIS_MODULE,
+	.iterate = hmdfs_iterate_local,
+	.open = hmdfs_dir_open_local,
+	.release = hmdfs_dir_release_local,
+	.fsync = hmdfs_fsync_local,
+};
+
 const struct file_operations hmdfs_dir_ops_share = {
 	.owner = THIS_MODULE,
 	.iterate = hmdfs_iterate_local,
diff --git a/fs/hmdfs/hmdfs.h b/fs/hmdfs/hmdfs.h
index 9157c55ba..0c5cce32e 100644
--- a/fs/hmdfs/hmdfs.h
+++ b/fs/hmdfs/hmdfs.h
@@ -55,6 +55,13 @@
 
 #define HMDFS_READPAGES_NR_MAX	32
 
+#define HMDFS_SHARE_ITEM_TIMEOUT_S 60
+#define HMDFS_SHARE_ITEMS_MAX 4
+
+#define HMDFS_IOC 0xf2
+#define HMDFS_IOC_SET_SHARE_PATH	_IOW(HMDFS_IOC, 1, \
+						struct hmdfs_share_control)
+
 #define HMDFS_CID_SIZE 64
 
 enum {
@@ -91,10 +98,18 @@ struct hmdfs_syncfs_info {
 	spinlock_t list_lock;
 };
 
+struct hmdfs_share_item {
+	struct file *file;
+	struct qstr relative_path;
+	char cid[HMDFS_CID_SIZE];
+	unsigned long timeout;
+	struct kref ref;
+	struct list_head list;
+};
+
 struct hmdfs_share_table {
 	struct list_head item_list_head;
 	spinlock_t item_list_lock;
-	struct workqueue_struct *share_item_timeout_wq;
 	int item_cnt;
 	int max_cnt;
 };
@@ -206,6 +221,11 @@ struct hmdfs_sb_info {
 	unsigned int user_id;
 };
 
+struct hmdfs_share_control {
+	__u32 src_fd;
+	char cid[HMDFS_CID_SIZE];
+};
+
 static inline struct hmdfs_sb_info *hmdfs_sb(struct super_block *sb)
 {
 	return sb->s_fs_info;
@@ -306,6 +326,19 @@ static inline bool qstr_eq(const struct qstr *q1, const struct qstr *q2)
 	return q1->len == q2->len && !strncmp(q1->name, q2->name, q2->len);
 }
 
+bool hmdfs_is_share_file(struct file *file);
+
+bool hmdfs_is_share_item_still_valid(struct hmdfs_share_item *item);
+
+inline void release_share_item(struct hmdfs_share_item *item);
+
+void hmdfs_remove_share_item(struct kref *ref);
+
+struct hmdfs_share_item *hmdfs_lookup_share_item(struct hmdfs_share_table *st,
+						struct qstr *cur_relative_path);
+
+inline void set_item_timeout(struct hmdfs_share_item *item);
+
 /*****************************************************************************
  * log print helpers
  *****************************************************************************/
diff --git a/fs/hmdfs/hmdfs_server.c b/fs/hmdfs/hmdfs_server.c
index dcb15c8b6..ccf8170b9 100644
--- a/fs/hmdfs/hmdfs_server.c
+++ b/fs/hmdfs/hmdfs_server.c
@@ -16,7 +16,6 @@
 #include "authority/authentication.h"
 #include "hmdfs.h"
 #include "hmdfs_dentryfile.h"
-#include "hmdfs_share.h"
 #include "hmdfs_trace.h"
 #include "server_writeback.h"
 #include "comm/node_cb.h"
@@ -100,6 +99,28 @@ struct file *hmdfs_open_path(struct hmdfs_sb_info *sbi, const char *path)
 	return file;
 }
 
+inline bool is_dst_device(char *src_cid, char *dst_cid)
+{
+	return strncmp(src_cid, dst_cid, HMDFS_CID_SIZE) == 0 ? true : false;
+}
+
+void hmdfs_clear_share_item_offline(struct hmdfs_peer *conn)
+{
+	struct hmdfs_sb_info *sbi = conn->sbi;
+	struct hmdfs_share_item *item, *tmp;
+
+	spin_lock(&sbi->share_table.item_list_lock);
+	list_for_each_entry_safe(item, tmp, &sbi->share_table.item_list_head,
+				list) {
+		if (is_dst_device(item->cid, conn->cid)) {
+			list_del(&item->list);
+			release_share_item(item);
+			sbi->share_table.item_cnt--;
+		}
+	}
+	spin_unlock(&sbi->share_table.item_list_lock);
+}
+
 inline void hmdfs_close_path(struct file *file)
 {
 	fput(file);
@@ -225,11 +246,30 @@ static int check_sec_level(struct hmdfs_peer *node, const char *file_name)
 	return ret;
 }
 
+static int hmdfs_check_share_access_permission(struct hmdfs_sb_info *sbi,
+		const char *filename, char *cid, struct hmdfs_share_item **item)
+{
+	struct qstr candidate = QSTR_INIT(filename, strlen(filename));
+	int ret = -ENOENT;
+
+	spin_lock(&sbi->share_table.item_list_lock);
+	*item = hmdfs_lookup_share_item(&sbi->share_table, &candidate);
+	if (*item && is_dst_device((*item)->cid, cid)) {
+		spin_unlock(&sbi->share_table.item_list_lock);
+		return 0;
+	} else
+		*item = NULL;
+	spin_unlock(&sbi->share_table.item_list_lock);
+
+	return ret;
+}
+
 static struct file *hmdfs_open_file(struct hmdfs_peer *con,
 				    const char *filename, uint8_t file_type,
 				    int *file_id)
 {
 	struct file *file = NULL;
+	struct hmdfs_share_item *item = NULL;
 	int err = 0;
 	int id;
 
@@ -245,26 +285,27 @@ static struct file *hmdfs_open_file(struct hmdfs_peer *con,
 
 	if (hm_isshare(file_type)) {
 		err = hmdfs_check_share_access_permission(con->sbi,
-							filename, con->cid);
+					filename, con->cid, &item);
 		if (err)
 			return ERR_PTR(err);
 	}
 	file = hmdfs_open_path(con->sbi, filename);
 
-	if (IS_ERR(file)) {
-		reset_item_opened_status(con->sbi, filename);
+	if (IS_ERR(file))
 		return file;
-	}
 
 	id = insert_file_into_conn(con, file);
 	if (id < 0) {
 		hmdfs_err("file_id alloc failed! err=%d", id);
-		reset_item_opened_status(con->sbi, filename);
 		hmdfs_close_path(file);
 		return ERR_PTR(id);
 	}
 	*file_id = id;
 
+	/* get item to avoid timeout */
+	if (item)
+		kref_get(&item->ref);
+
 	return file;
 }
 
@@ -637,6 +678,42 @@ void hmdfs_server_atomic_open(struct hmdfs_peer *con,
 	kfree(resp);
 }
 
+void hmdfs_close_share_item(struct hmdfs_sb_info *sbi, struct file *file,
+			    char *cid)
+{
+	struct qstr relativepath;
+	const char *path_name;
+	struct hmdfs_share_item *item = NULL;
+
+	path_name = hmdfs_get_dentry_relative_path(file->f_path.dentry);
+	if (unlikely(!path_name)) {
+		hmdfs_err("get dentry relative path error");
+		return;
+	}
+
+	relativepath.name = path_name;
+	relativepath.len = strlen(path_name);
+
+	item = hmdfs_lookup_share_item(&sbi->share_table, &relativepath);
+
+	if (item) {
+		if (unlikely(!is_dst_device(item->cid, cid))) {
+			hmdfs_err("item not right");
+			goto err_out;
+		}
+
+		if (unlikely(kref_read(&item->ref) == 1))
+			hmdfs_err("item ref error");
+
+		set_item_timeout(item);
+		kref_put(&item->ref, hmdfs_remove_share_item);
+	} else
+		hmdfs_err("cannot get share item %s", relativepath.name);
+
+err_out:
+	kfree(path_name);
+}
+
 void hmdfs_server_release(struct hmdfs_peer *con, struct hmdfs_head_cmd *cmd,
 			  void *data)
 {
diff --git a/fs/hmdfs/inode_local.c b/fs/hmdfs/inode_local.c
index 2470c5f81..04388c808 100644
--- a/fs/hmdfs/inode_local.c
+++ b/fs/hmdfs/inode_local.c
@@ -18,7 +18,6 @@
 #include "hmdfs_client.h"
 #include "hmdfs_dentryfile.h"
 #include "hmdfs_device_view.h"
-#include "hmdfs_share.h"
 #include "hmdfs_trace.h"
 
 extern struct kmem_cache *hmdfs_dentry_cachep;
@@ -62,10 +61,13 @@ static inline void set_sharefile_flag(struct hmdfs_dentry_info *gdi)
 	gdi->file_type = HM_SHARE;
 }
 
-static void check_and_fixup_share_ops(struct inode *inode,
+static inline void check_and_fixup_share_ops(struct inode *inode,
 					const char *name)
 {
-	if (is_share_dir(inode, name)) {
+	const char *share_dir = ".share";
+
+	if (S_ISDIR(inode->i_mode) &&
+		!strncmp(name, share_dir, strlen(share_dir))) {
 		inode->i_op = &hmdfs_dir_inode_ops_share;
 		inode->i_fop = &hmdfs_dir_ops_share;
 	}
@@ -798,6 +800,39 @@ static ssize_t hmdfs_local_listxattr(struct dentry *dentry, char *list,
 
 	return res;
 }
+
+int hmdfs_get_path_from_share_table(struct hmdfs_sb_info *sbi,
+			struct dentry *cur_dentry, struct path *src_path)
+{
+	struct hmdfs_share_item *item;
+	const char *path_name;
+	struct qstr relative_path;
+	int err = 0;
+
+	path_name = hmdfs_get_dentry_relative_path(cur_dentry);
+	if (unlikely(!path_name)) {
+		err = -ENOMEM;
+		goto err_out;
+	}
+	relative_path.name = path_name;
+	relative_path.len = strlen(path_name);
+
+	spin_lock(&sbi->share_table.item_list_lock);
+	item = hmdfs_lookup_share_item(&sbi->share_table, &relative_path);
+	if (!item) {
+		spin_unlock(&sbi->share_table.item_list_lock);
+		err = -ENOENT;
+		goto err_out;
+	}
+	*src_path = item->file->f_path;
+	path_get(src_path);
+
+	kfree(path_name);
+	spin_unlock(&sbi->share_table.item_list_lock);
+err_out:
+	return err;
+}
+
 struct dentry *hmdfs_lookup_share(struct inode *parent_inode,
 				struct dentry *child_dentry, unsigned int flags)
 {
@@ -820,7 +855,7 @@ struct dentry *hmdfs_lookup_share(struct inode *parent_inode,
 		goto err_out;
 	}
 
-	err = get_path_from_share_table(sbi, child_dentry, &src_path);
+	err = hmdfs_get_path_from_share_table(sbi, child_dentry, &src_path);
 	if (err) {
 		ret = ERR_PTR(err);
 		goto err_out;
@@ -848,6 +883,8 @@ struct dentry *hmdfs_lookup_share(struct inode *parent_inode,
 	check_and_fixup_ownership(parent_inode, child_inode);
 
 err_out:
+	if (!err)
+		hmdfs_set_time(child_dentry, jiffies);
 	trace_hmdfs_lookup_share_end(parent_inode, child_dentry, err);
 	return ret;
 }
diff --git a/fs/hmdfs/inode_remote.c b/fs/hmdfs/inode_remote.c
index 73d459bf4..0a4493455 100644
--- a/fs/hmdfs/inode_remote.c
+++ b/fs/hmdfs/inode_remote.c
@@ -14,7 +14,6 @@
 #include "hmdfs.h"
 #include "hmdfs_client.h"
 #include "hmdfs_dentryfile.h"
-#include "hmdfs_share.h"
 #include "hmdfs_trace.h"
 #include "authority/authentication.h"
 #include "stash.h"
@@ -403,6 +402,19 @@ struct inode *fill_inode_remote(struct super_block *sb, struct hmdfs_peer *con,
 	return ERR_PTR(ret);
 }
 
+static bool in_share_dir(struct dentry *child_dentry)
+{
+	struct dentry *parent_dentry = dget_parent(child_dentry);
+	bool ret = false;
+	const char *share_dir = ".share";
+
+	if (!strncmp(parent_dentry->d_name.name, share_dir, strlen(share_dir)))
+		ret = true;
+
+	dput(parent_dentry);
+	return ret;
+}
+
 static struct dentry *hmdfs_lookup_remote_dentry(struct inode *parent_inode,
 						 struct dentry *child_dentry,
 						 int flags)
diff --git a/fs/hmdfs/main.c b/fs/hmdfs/main.c
index d0a41061b..f692cfa89 100644
--- a/fs/hmdfs/main.c
+++ b/fs/hmdfs/main.c
@@ -27,7 +27,6 @@
 #include "comm/socket_adapter.h"
 #include "hmdfs_merge_view.h"
 #include "server_writeback.h"
-#include "hmdfs_share.h"
 
 #include "comm/node_cb.h"
 #include "stash.h"
@@ -242,7 +241,6 @@ void hmdfs_put_super(struct super_block *sb)
 	hmdfs_cfn_destroy(sbi);
 	hmdfs_unregister_sysfs(sbi);
 	hmdfs_connections_stop(sbi);
-	hmdfs_clear_share_table(sbi);
 	hmdfs_destroy_server_writeback(sbi);
 	hmdfs_exit_stash(sbi);
 	atomic_dec(&lower_sb->s_active);
@@ -662,6 +660,14 @@ static void hmdfs_init_cmd_timeout(struct hmdfs_sb_info *sbi)
 	set_cmd_timeout(sbi, F_LISTXATTR, TIMEOUT_COMMON);
 }
 
+static void init_share_table(struct hmdfs_sb_info *sbi)
+{
+	spin_lock_init(&sbi->share_table.item_list_lock);
+	INIT_LIST_HEAD(&sbi->share_table.item_list_head);
+	sbi->share_table.item_cnt = 0;
+	sbi->share_table.max_cnt = HMDFS_SHARE_ITEMS_MAX;
+}
+
 static int hmdfs_init_sbi(struct hmdfs_sb_info *sbi)
 {
 	int ret;
@@ -712,7 +718,7 @@ static int hmdfs_init_sbi(struct hmdfs_sb_info *sbi)
 	mutex_init(&sbi->connections.node_lock);
 	INIT_LIST_HEAD(&sbi->connections.node_list);
 
-	hmdfs_init_share_table(sbi);
+	init_share_table(sbi);
 	init_waitqueue_head(&sbi->async_readdir_wq);
 	INIT_LIST_HEAD(&sbi->async_readdir_msg_list);
 	INIT_LIST_HEAD(&sbi->async_readdir_work_list);
diff --git a/fs/proc/meminfo.c b/fs/proc/meminfo.c
index 248e0afea..887a5532e 100644
--- a/fs/proc/meminfo.c
+++ b/fs/proc/meminfo.c
@@ -122,10 +122,6 @@ static int meminfo_proc_show(struct seq_file *m, void *v)
 	show_val_kb(m, "VmallocChunk:   ", 0ul);
 	show_val_kb(m, "Percpu:         ", pcpu_nr_pages());
 
-#ifdef CONFIG_PAGE_TRACING
-	show_val_kb(m, "Skb:            ", global_zone_page_state(NR_SKB_PAGES));
-#endif
-
 #ifdef CONFIG_MEMORY_FAILURE
 	seq_printf(m, "HardwareCorrupted: %5lu kB\n",
 		   atomic_long_read(&num_poisoned_pages) << (PAGE_SHIFT - 10));
@@ -150,11 +146,6 @@ static int meminfo_proc_show(struct seq_file *m, void *v)
 		    global_zone_page_state(NR_FREE_CMA_PAGES));
 #endif
 
-#ifdef CONFIG_PAGE_TRACING
-	seq_puts(m, "GLTrack:               - kB\n");
-	show_val_kb(m, "ZspageUsed:	", global_zone_page_state(NR_ZSPAGES));
-#endif
-
 	hugetlb_report_meminfo(m);
 
 	arch_report_meminfo(m);
diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c
index 46407d4e0..0e22a47e8 100644
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@ -1,7 +1,6 @@
 // SPDX-License-Identifier: GPL-2.0
 #include <linux/pagewalk.h>
 #include <linux/vmacache.h>
-#include <linux/mm_inline.h>
 #include <linux/hugetlb.h>
 #include <linux/huge_mm.h>
 #include <linux/mount.h>
@@ -309,7 +308,7 @@ show_map_vma(struct seq_file *m, struct vm_area_struct *vma)
 
 	name = arch_vma_name(vma);
 	if (!name) {
-		struct anon_vma_name *anon_name;
+		const char *anon_name;
 
 		if (!mm) {
 			name = "[vdso]";
@@ -327,10 +326,10 @@ show_map_vma(struct seq_file *m, struct vm_area_struct *vma)
 			goto done;
 		}
 
-		anon_name = anon_vma_name(vma);
+		anon_name = vma_anon_name(vma);
 		if (anon_name) {
 			seq_pad(m, ' ');
-			seq_printf(m, "[anon:%s]", anon_name->name);
+			seq_printf(m, "[anon:%s]", anon_name);
 		}
 	}
 
diff --git a/fs/signalfd.c b/fs/signalfd.c
index b94fb5f81..456046e15 100644
--- a/fs/signalfd.c
+++ b/fs/signalfd.c
@@ -35,7 +35,17 @@
 
 void signalfd_cleanup(struct sighand_struct *sighand)
 {
-	wake_up_pollfree(&sighand->signalfd_wqh);
+	wait_queue_head_t *wqh = &sighand->signalfd_wqh;
+	/*
+	 * The lockless check can race with remove_wait_queue() in progress,
+	 * but in this case its caller should run under rcu_read_lock() and
+	 * sighand_cachep is SLAB_TYPESAFE_BY_RCU, we can safely return.
+	 */
+	if (likely(!waitqueue_active(wqh)))
+		return;
+
+	/* wait_queue_entry_t->func(POLLFREE) should do remove_wait_queue() */
+	wake_up_poll(wqh, EPOLLHUP | POLLFREE);
 }
 
 struct signalfd_ctx {
diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c
index 4229881e1..4a864090e 100644
--- a/fs/userfaultfd.c
+++ b/fs/userfaultfd.c
@@ -15,7 +15,6 @@
 #include <linux/sched/signal.h>
 #include <linux/sched/mm.h>
 #include <linux/mm.h>
-#include <linux/mm_inline.h>
 #include <linux/poll.h>
 #include <linux/slab.h>
 #include <linux/seq_file.h>
@@ -870,7 +869,7 @@ static int userfaultfd_release(struct inode *inode, struct file *file)
 				 new_flags, vma->anon_vma,
 				 vma->vm_file, vma->vm_pgoff,
 				 vma_policy(vma),
-				 NULL_VM_UFFD_CTX, anon_vma_name(vma));
+				 NULL_VM_UFFD_CTX, vma_anon_name(vma));
 		if (prev)
 			vma = prev;
 		else
@@ -1412,7 +1411,7 @@ static int userfaultfd_register(struct userfaultfd_ctx *ctx,
 				 vma->anon_vma, vma->vm_file, vma->vm_pgoff,
 				 vma_policy(vma),
 				 ((struct vm_userfaultfd_ctx){ ctx }),
-				 anon_vma_name(vma));
+				 vma_anon_name(vma));
 		if (prev) {
 			vma = prev;
 			goto next;
@@ -1582,7 +1581,7 @@ static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,
 		prev = vma_merge(mm, prev, start, vma_end, new_flags,
 				 vma->anon_vma, vma->vm_file, vma->vm_pgoff,
 				 vma_policy(vma),
-				 NULL_VM_UFFD_CTX, anon_vma_name(vma));
+				 NULL_VM_UFFD_CTX, vma_anon_name(vma));
 		if (prev) {
 			vma = prev;
 			goto next;
